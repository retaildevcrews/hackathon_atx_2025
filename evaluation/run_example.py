"""Example script showing how to run the evaluation framework."""

import os
import logging
from evaluation import run_evaluation

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def main():
    """Run evaluation example."""
    
    # Set environment variables (in practice, use .env file or config)
    os.environ.setdefault("JUDGE_MODEL", "gpt-4")
    os.environ.setdefault("AGENT_ENDPOINT", "http://localhost:8000/evaluate")
    os.environ.setdefault("MIN_ACCEPTABLE_SCORE", "70.0")
    
    try:
        # Run evaluation
        print("ğŸ” Starting evaluation...")
        results = run_evaluation(
            dataset_path="evaluation/dataset/test_cases.jsonl",
            output_dir="evaluation/reports",
            judge_model="gpt-4"  # Override default
        )
        
        # Print summary
        metrics = results["metrics"]
        summary = metrics.get("summary", {})
        
        print(f"\nâœ… Evaluation complete!")
        print(f"ğŸ“Š Dataset size: {results['dataset_size']} test cases")
        print(f"ğŸ¯ Mean judge score: {summary.get('mean_judge_score', 0):.1f}")
        print(f"ğŸ“ˆ Score range: {summary.get('min_judge_score', 0):.1f} - {summary.get('max_judge_score', 0):.1f}")
        
        # Show performance distribution
        distribution = summary.get("score_distribution", {})
        print(f"\nğŸ“ˆ Performance distribution:")
        for category, count in distribution.items():
            print(f"  {category}: {count} cases")
        
        # Show recommendations
        recommendations = metrics.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ Recommendations:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
        # Show low-performing cases
        low_score_cases = [
            r for r in results["results"] 
            if r["judge_results"][0].get("overall_judge_score", 0) < 70
        ]
        
        if low_score_cases:
            print(f"\nâš ï¸  Cases needing attention ({len(low_score_cases)}):")
            for case in low_score_cases[:3]:  # Show top 3
                doc_id = case["test_case"]["document_id"]
                score = case["judge_results"][0].get("overall_judge_score", 0)
                print(f"  - {doc_id}: {score:.1f}")
        
        print(f"\nğŸ“„ Detailed reports saved to: evaluation/reports/")
        
    except FileNotFoundError as e:
        print(f"âŒ Dataset file not found: {e}")
        print("ğŸ’¡ Make sure the test dataset exists at evaluation/dataset/test_cases.jsonl")
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        logging.exception("Evaluation error")


if __name__ == "__main__":
    main()